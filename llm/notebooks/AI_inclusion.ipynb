{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmH_MZ2uuGts",
        "outputId": "b541b734-f538-44f0-acc1-2d5d643027d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain)\n",
            "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.116-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.116-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, orjson, mypy-extensions, marshmallow, jsonpointer, jiter, h11, typing-inspect, pydeck, jsonpatch, httpcore, gitdb, httpx, gitpython, dataclasses-json, accelerate, openai, langsmith, streamlit, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.33.0\n",
            "    Uninstalling accelerate-0.33.0:\n",
            "      Successfully uninstalled accelerate-0.33.0\n",
            "Successfully installed accelerate-0.34.2 dataclasses-json-0.6.7 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.16 langchain-core-0.2.38 langchain-text-splitters-0.2.4 langchain_community-0.2.16 langsmith-0.1.116 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.44.0 orjson-3.10.7 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 typing-inspect-0.9.0 watchdog-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain_community openai streamlit transformers accelerate\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8pZmcmHvU4q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbJ_yGFOtNrH"
      },
      "source": [
        "*1. Data Ingestion*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo7aIzhos-8A",
        "outputId": "d021aa40-1965-424e-be02-810114d18e74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "# Now fetch the content from the government site\n",
        "gov_site_url = \"https://immi.homeaffairs.gov.au/citizenship/become-a-citizen\"\n",
        "gov_response = requests.get(gov_site_url)\n",
        "gov_soup = BeautifulSoup(gov_response.content, 'html.parser')\n",
        "\n",
        "# Extract the content you need\n",
        "content = \"\\n\".join([p.get_text() for p in gov_soup.find_all(\"p\")])\n",
        "\n",
        "#add vector database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_PMzpX_eefI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivhmuiDnee8m"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoxsNa7ltVEw"
      },
      "source": [
        "*2. Use openAI for Simplification and Summarization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcX3_OlvtZpL",
        "outputId": "8b31e283-3d20-4829-cce5-42e67edf016b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Title: Guidelines for Writing in Plain English\n",
            "\n",
            "Introduction\n",
            "\n",
            "Writing in plain English is an essential skill for effectively communicating with your audience. It involves using simple, clear language that everyone can understand. The following guidelines will help you to write in plain English.\n",
            "\n",
            "Why use plain English?\n",
            "\n",
            "Plain English is important because it makes your message accessible to a wider audience. It removes confusion, reduces jargon, and ensures that your message is understood by all readers, regardless of their level of education or background.\n",
            "\n",
            "Principles of plain English\n",
            "\n",
            "To write in plain English, you should follow these principles:\n",
            "\n",
            "1. Use short sentences: Short sentences are easier to read and understand.\n",
            "\n",
            "2. Use everyday words: Avoid using technical jargon or complex language. Instead, use simple, everyday words that everyone can understand.\n",
            "\n",
            "3. Be concise: Stick to the main points and avoid unnecessary details. This will make your writing more clear and to the point.\n",
            "\n",
            "4. Use active voice: Active voice is more direct and engaging than passive voice. It also helps to clarify who is responsible for an action.\n",
            "\n",
            "5. Use headings and subheadings: Headings and subheadings help to break up the text and make it easier to navigate. They also provide a quick overview of the content.\n",
            "\n",
            "6.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(temperature=0.7, openai_api_key=\"\")\n",
        "\n",
        "# Define a template to simplify text\n",
        "template = \"\"\"\n",
        "Rewrite the following content to follow the Australian Government Style Manual:\n",
        "- Use plain English.\n",
        "- Improve headings\n",
        "Text: {content}\n",
        "\"\"\"\n",
        "# parse the style guide into it - headings improvement\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"content\"])\n",
        "# chain = LLMChain(llm=llm, prompt=prompt)\n",
        "simplified_content = llm(prompt.format(content=content))\n",
        "print(simplified_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5oybOce8F5f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdIVsXCdtnco"
      },
      "source": [
        "*3. Interactive Q&A System with LangChain Conversational Agents*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttwu_NK6tqei",
        "outputId": "fe2d2ac7-eb62-43f7-d388-2d0c9c21f761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Applying for Australian citizenship is a relatively straightforward process. First, you will need to determine if you are eligible for citizenship. This can depend on factors such as your current visa status, age, and length of time residing in Australia. Once you have confirmed your eligibility, you can fill out an online application through the Australian Department of Home Affairs website. The application will require you to provide personal information, such as your name, address, and date of birth. You will also need to provide evidence of your identity, such as a passport or birth certificate. Additionally, you will need to complete a citizenship test and attend an interview to assess your knowledge of Australian society, values, and customs. It is important to note that there is a fee for the application process, which can vary depending on your circumstances.\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# Simulate user query\n",
        "user_query = \"How can I apply for Australian citizenship?\"\n",
        "response = conversation.run(user_query)\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tCgAapFtuPN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irp8bwn38JQ-"
      },
      "source": [
        "4. Another Agent as Senior Editor to verfiy if the generated content fit the style guide with measurement.\n",
        "we can update the process by using GPT-3.5 or GPT-4 to handle the interactions between the Learner Agent and the Senior Editor Agent. Here’s how we can set up the system using GPT for both simplifying content and evaluating it for adherence to style guidelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa7LJOS6LSio",
        "outputId": "0b9a1b52-6360-4437-b0e8-f9f091166651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simplified Content:\n",
            " \n",
            "H1: How to Get a Driver's License\n",
            "H2: Passing the Theory Test\n",
            "H3: Understanding Road Rules\n",
            "H3: Scheduling Your Theory Test\n",
            "H2: Passing the Driving Test\n",
            "H3: Preparing for the Test\n",
            "H3: Scheduling Your Driving Test\n",
            "\n",
            "\n",
            "Senior Editor Feedback:\n",
            " \n",
            "Overall, the new headings are clear and organized. They follow a logical flow and provide specific information for each section. The use of subheadings under the main headings also helps break down the information into smaller chunks, making it easier for readers to digest. \n",
            "\n",
            "One suggestion for improvement would be to use consistent heading levels. For example, \"Understanding Road Rules\" is currently labeled as an H3, but it could be made into an H2 to match the other main headings. This would also create a clearer hierarchy and make the content easier to navigate.\n",
            "\n",
            "Additionally, it would be helpful to include more descriptive headings. For instance, instead of just \"Preparing for the Test,\" it could be expanded to \"Preparing for the Driving Test.\" This would give readers a better idea of what to expect in that section and make it more relevant to their specific needs.\n",
            "\n",
            "Overall, the use of headings has greatly improved the organization and readability of the content. It effectively guides readers through the process of getting a driver's license and helps them find the information they need. Great job!\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize GPT-3.5 or GPT-4 models for both Learner Agent and Senior Editor Agent\n",
        "gpt_learner = OpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"\")  # Learner Agent (simplifies content)\n",
        "gpt_editor = OpenAI(model=\"gpt-4\", openai_api_key=\"\")  # Senior Editor Agent (evaluates content)\n",
        "\n",
        "# Define Technical Writer Agent (simplifies content)\n",
        "def simplify_content(content):\n",
        "    simplify_prompt = PromptTemplate(\n",
        "        input_variables=[\"content\"],\n",
        "        template=\"Simplify the following content, such as improve headings for better understanding:\\n{content}\"\n",
        "    )\n",
        "\n",
        "    # chain = LLMChain(llm=gpt_learner, prompt=simplify_prompt)\n",
        "    # simplified_content = chain.run(content)\n",
        "    simplified_content = llm(simplify_prompt.format(content=content))\n",
        "    return simplified_content\n",
        "\n",
        "# Define Senior Editor Agent (evaluates simplified content)\n",
        "def senior_editor(content, style_guide):\n",
        "    style_prompt = PromptTemplate(\n",
        "        input_variables=[\"content\", \"style_guide\"],\n",
        "        template=(\n",
        "            \"Review the following content according by an editor who has improved the headings \"\n",
        "            \"Can you please provide some feedback:\\n\"\n",
        "            \"Content: {content}\\n\"\n",
        "            \"Style Guide: {style_guide}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # chain = LLMChain(llm=gpt_editor, prompt=style_prompt)\n",
        "    # feedback = chain.run({\"content\": content, \"style_guide\": style_guide})\n",
        "    feedback = llm(style_prompt.format(content=content, style_guide=style_guide))\n",
        "    return feedback\n",
        "\n",
        "# Sample Australian Style Manual guidelines\n",
        "style_guide = \"\"\"\n",
        "Use headings help users scan content and find what they need.\n",
        "\"\"\"\n",
        "\n",
        "# Simulated workflow between Learner Agent and Senior Editor Agent\n",
        "def collaborative_revision_process(content):\n",
        "    # Step 1: Learner Agent generates simplified content\n",
        "    simplified_content = simplify_content(content)\n",
        "    print(\"Simplified Content:\\n\", simplified_content)\n",
        "\n",
        "    # Step 2: Senior Editor Agent verifies and provides feedback\n",
        "    feedback = senior_editor(simplified_content, style_guide)\n",
        "    print(\"\\n\\nSenior Editor Feedback:\\n\", feedback)\n",
        "\n",
        "    # Step 3: Learner Agent revises based on feedback (in real system, would resubmit)\n",
        "    revised_content = simplified_content + \" [Revision applied based on feedback]\"\n",
        "\n",
        "    return revised_content\n",
        "\n",
        "# Example content to simplify from government site\n",
        "gov_site_content = \"\"\"\n",
        "H1: Apply for a drivers licence\n",
        "H2: Pass the theory test\n",
        "H3: Learn the road rules\n",
        "H3: Book a theory test\n",
        "H2: Pass the driving test\n",
        "H3: Practise for the test\n",
        "H3: Book the driving test\n",
        "\"\"\"\n",
        "\n",
        "# Run the process\n",
        "final_output = collaborative_revision_process(gov_site_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaZLzfTkOpWB"
      },
      "source": [
        "*5.AutoGen Version*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDci8C2nPUri",
        "outputId": "e836bf91-27a7-4500-aec6-cf8d06e7de27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: autogen in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from autogen) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from autogen) (7.1.0)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from autogen) (2.2.0)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.26.4)\n",
            "Collecting openai>=1.3 (from autogen)\n",
            "  Using cached openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from autogen) (1.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen) (2.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from autogen) (0.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->autogen) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (2.20.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen) (3.3.2)\n",
            "Using cached openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.44.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install autogen -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "B8NlJ_FrOriT",
        "outputId": "6a306f34-3004-4a08-af05-6c765ec03ce6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'AutoGenAgent' from 'autogen' (/usr/local/lib/python3.10/dist-packages/autogen/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautogen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoGenAgent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define the agents\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLearnerAgent\u001b[39;00m(AutoGenAgent):\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoGenAgent' from 'autogen' (/usr/local/lib/python3.10/dist-packages/autogen/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from autogen import AutoGenAgent\n",
        "\n",
        "# Define the agents\n",
        "class LearnerAgent(AutoGenAgent):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.role = \"Learner Agent (Simplifier)\"\n",
        "\n",
        "    def perform_task(self, content):\n",
        "        prompt = f\"Simplify the following bureaucratic text into plain English:\\n{content}\"\n",
        "        simplified_content = self.generate_response(prompt)\n",
        "        return simplified_content\n",
        "\n",
        "\n",
        "class SeniorEditorAgent(AutoGenAgent):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.role = \"Senior Editor Agent (Verifier)\"\n",
        "\n",
        "    def perform_task(self, content, style_guide):\n",
        "        prompt = f\"Verify the following simplified content against the style guide and provide feedback:\\n\"\\\n",
        "                 f\"Content: {content}\\nStyle Guide: {style_guide}\"\n",
        "        feedback = self.generate_response(prompt)\n",
        "        return feedback\n",
        "\n",
        "\n",
        "# Define style guide (you can extend this with detailed guidelines)\n",
        "style_guide = \"\"\"\n",
        "1. Use plain English.\n",
        "2. Write in an active voice.\n",
        "3. Avoid unnecessary jargon.\n",
        "4. Ensure inclusivity and accessibility.\n",
        "\"\"\"\n",
        "\n",
        "# Example government content\n",
        "gov_content = \"\"\"\n",
        "Citizenship gives you the opportunity to participate in your community, vote, and feel a sense of belonging.\n",
        "However, understanding the legal requirements of citizenship can be challenging due to complex language and bureaucratic processes.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4: Set up communication between the agents\n",
        "def simplified_content_workflow(gov_content, style_guide):\n",
        "    # Initialize the learner agent and senior editor agent\n",
        "    learner_agent = LearnerAgent(model=\"gpt-3.5-turbo\")  # Learner agent using GPT-3.5 Turbo\n",
        "    senior_editor_agent = SeniorEditorAgent(model=\"gpt-4\")  # Senior Editor using GPT-4\n",
        "\n",
        "    # Step 1: Learner agent simplifies the content\n",
        "    simplified_content = learner_agent.perform_task(gov_content)\n",
        "    print(f\"Simplified Content:\\n{simplified_content}\\n\")\n",
        "\n",
        "    # Step 2: Senior Editor agent evaluates and provides feedback\n",
        "    feedback = senior_editor_agent.perform_task(simplified_content, style_guide)\n",
        "    print(f\"Senior Editor Feedback:\\n{feedback}\\n\")\n",
        "\n",
        "    # Step 3: Learner agent revises the content based on feedback (optional, as a loop)\n",
        "    revised_content = simplified_content + \" [Revision applied based on feedback]\"\n",
        "\n",
        "    return revised_content\n",
        "\n",
        "# Run the workflow\n",
        "final_output = simplified_content_workflow(gov_content, style_guide)\n",
        "print(f\"Final Output:\\n{final_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWAr7FkkMfln"
      },
      "source": [
        "*6. Combine with Vector Database*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GupzpgdOM0qe",
        "outputId": "c379ca94-6e72-4449-90d2-a4e4d21bbb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0heu5SuGMUhJ"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "import openai\n",
        "\n",
        "# Set up OpenAI API Key\n",
        "openai.api_key = ''\n",
        "\n",
        "# Fetch the content from the government site\n",
        "gov_site_url = \"https://immi.homeaffairs.gov.au/citizenship/become-a-citizen\"\n",
        "gov_response = requests.get(gov_site_url)\n",
        "gov_soup = BeautifulSoup(gov_response.content, 'html.parser')\n",
        "\n",
        "# Extract the content you need\n",
        "# The original code was only extracting text from <p> tags.\n",
        "# This website has content in other tags like <h2>, <h3>, etc.\n",
        "# The code below extracts text from all tags\n",
        "content = \"\\n\".join([tag.get_text() for tag in gov_soup.find_all()])\n",
        "\n",
        "# Step 1: Convert the text into chunks for embedding\n",
        "texts = content.split(\"\\n\")  # Simple chunking by paragraphs (or customize this)\n",
        "\n",
        "# Check if any text was extracted\n",
        "if not texts:\n",
        "    raise ValueError(\"No content was extracted from the website.\")\n",
        "\n",
        "# Step 2: Initialize the embedding model\n",
        "embedding_model = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "\n",
        "# Step 3: Generate embeddings for the text chunks\n",
        "embeddings = embedding_model.embed_documents(texts)\n",
        "\n",
        "# Check if embeddings were generated\n",
        "if not embeddings:\n",
        "    raise ValueError(\"Embedding generation failed. No embeddings created.\")\n",
        "\n",
        "# Step 4: Initialize the FAISS index for vector storage\n",
        "dimension = len(embeddings[0])  # Embedding vector dimension\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
        "\n",
        "# Create a docstore for storing the texts\n",
        "docstore = {i: text for i, text in enumerate(texts)}\n",
        "\n",
        "# Create a mapping from FAISS index to docstore IDs\n",
        "index_to_docstore_id = {i: i for i in range(len(texts))}\n",
        "\n",
        "# Initialize FAISS with the docstore and index_to_docstore_id\n",
        "faiss_index = FAISS(embedding_model.embed_query, index, docstore, index_to_docstore_id)\n",
        "\n",
        "# Step 5: Add the document embeddings to FAISS vector store\n",
        "faiss_index.add_texts(texts)\n",
        "\n",
        "# Function to query FAISS vector store\n",
        "def query_faiss_index(query):\n",
        "    # Convert the query to an embedding vector\n",
        "    query_embedding = embedding_model.embed_query(query)\n",
        "\n",
        "    # Perform similarity search on the vector store\n",
        "    results = faiss_index.similarity_search_with_score(query)\n",
        "\n",
        "    # Output the top result's content\n",
        "    top_result = results[0][0]\n",
        "    return top_result\n",
        "\n",
        "# Example: Querying content from the vector database, then processing it\n",
        "query = \"How to become an Australian citizen?\"\n",
        "gov_site_content = query_faiss_index(query)\n",
        "\n",
        "# Define Learner Agent (simplifies content)\n",
        "def simplify_content(content):\n",
        "    simplify_prompt = PromptTemplate(\n",
        "        input_variables=[\"content\"],\n",
        "        template=\"Simplify the following content, such as improve headings for better understanding:\\n{content}\"\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=gpt_learner, prompt=simplify_prompt)\n",
        "    simplified_content = chain.run(content)\n",
        "    return simplified_content\n",
        "\n",
        "# Define Senior Editor Agent (evaluates simplified content)\n",
        "def senior_editor(content, style_guide):\n",
        "    style_prompt = PromptTemplate(\n",
        "        input_variables=[\"content\", \"style_guide\"],\n",
        "        template=(\n",
        "            \"Review the following content according by an editor who has improved the headings \"\n",
        "            \"Can you please provide some feedback:\\n\"\n",
        "            \"\\nContent: {content}\\nStyle Guide: {style_guide}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(llm=gpt_editor, prompt=style_prompt)\n",
        "    feedback = chain.run({\"content\": content, \"style_guide\": style_guide})\n",
        "    return feedback\n",
        "\n",
        "# Simulated workflow between Learner Agent and Senior Editor Agent\n",
        "def collaborative_revision_process(content):\n",
        "    # Step 1: Learner Agent generates simplified content\n",
        "    simplified_content = simplify_content(content)\n",
        "    print(\"Simplified Content:\\n\", simplified_content)\n",
        "\n",
        "    # Step 2: Senior Editor Agent verifies and provides feedback\n",
        "    style_guide = \"Use headings to help users scan content and find what they need.\"\n",
        "    feedback = senior_editor(simplified_content, style_guide)\n",
        "    print(\"\\n\\nSenior Editor Feedback:\\n\", feedback)\n",
        "\n",
        "    # Step 3: Learner Agent revises based on feedback (in a real system, this would resubmit)\n",
        "    revised_content = simplified_content + \" [Revision applied based on feedback]\"\n",
        "\n",
        "    return revised_content\n",
        "\n",
        "# Run the collaborative revision process on the retrieved content\n",
        "final_output = collaborative_revision_process(gov_site_content)\n",
        "print(\"\\n\\nFinal Revised Content:\\n\", final_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
